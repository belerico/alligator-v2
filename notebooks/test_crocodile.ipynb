{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped collection: input_data\n",
      "All unwanted collections have been dropped.\n",
      "Onboarding unknown rows for dataset '56013d0b32224702aaba87e3d5af5929', table 'imdb_top_1000'\n",
      "Chunk 1: Processed 1000 rows (total: 1000) (26251.3 rows/sec)\n",
      "Data onboarding complete for dataset '56013d0b32224702aaba87e3d5af5929' and table 'imdb_top_1000'\n",
      "Onboarded 1000 rows in 0.04 seconds (26099.5 rows/sec)\n",
      "Found 1000 tasks to process.\n",
      "No more tasks to process.No more tasks to process.No more tasks to process.\n",
      "No more tasks to process.\n",
      "\n",
      "\n",
      "No more tasks to process.No more tasks to process.\n",
      "\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "Computing type-frequency features by randomly sampling 200 documents\n",
      "Computed type frequencies from 200 documents\n",
      "ML ranking progress: 0/1000 documents\n",
      "ML ranking progress: 0/939 documents\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "ML ranking progress: 256/1000 documents\n",
      "ML ranking progress: 256/939 documents\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step\n",
      "ML ranking progress: 509/1000 documents\n",
      "ML ranking complete: 509/1000 documents\n",
      "ML ranking progress: 491/939 documents\n",
      "ML ranking complete: 491/939 documents\n",
      "All tasks have been processed.\n",
      "Streaming 1000 documents to CSV...\n",
      "Processed 256/1000 rows...\n",
      "Processed 512/1000 rows...\n",
      "Processed 768/1000 rows...\n",
      "Processed 1000/1000 rows...\n",
      "Results saved to './tables/imdb_top_1000_output.csv'. Total rows: 1000\n",
      "Elapsed time: 1.7090901669998857\n",
      "Entity linking process completed.\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.clear_output(wait=True)\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from alligator import Alligator\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/imdb_top_1000.csv'\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "# Drop the entire crocodile_db database\n",
    "#client.drop_database(\"crocodile_db\")\n",
    "db = client[\"crocodile_db\"]\n",
    "\n",
    "# Drop all collections except 'bow_cache' and 'candidate_cache'\n",
    "collections_to_keep = [\"bow_cache\", \"candidate_cache\"]\n",
    "all_collections = db.list_collection_names()\n",
    "\n",
    "for collection in all_collections:\n",
    "    if collection not in collections_to_keep:\n",
    "        db[collection].drop()\n",
    "        print(f\"Dropped collection: {collection}\")\n",
    "\n",
    "print(\"All unwanted collections have been dropped.\")\n",
    "\n",
    "# Create an instance of the Alligator class\n",
    "crocodile_instance = Alligator(\n",
    "    input_csv=file_path,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    max_workers=8,\n",
    "    candidate_retrieval_limit=10,\n",
    "    max_candidates_in_result=3,\n",
    "    batch_size=256,\n",
    "    model_path=\"./alligator/models/default.h5\",\n",
    "    columns_type={\n",
    "    \"NE\": {\n",
    "        \"0\": \"OTHER\"\n",
    "    },\n",
    "    \"LIT\": {\n",
    "        \"1\": \"NUMBER\",\n",
    "        \"2\": \"NUMBER\",\n",
    "        \"3\": \"STRING\",\n",
    "        \"4\": \"NUMBER\",\n",
    "        \"5\": \"STRING\"\n",
    "    },\n",
    "    # \"IGNORED\" : [\"6\", \"9\", \"10\", \"7\", \"8\"]\n",
    "}\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "tic = time.perf_counter()\n",
    "crocodile_instance.run()\n",
    "toc = time.perf_counter()\n",
    "print(\"Elapsed time:\", toc - tic)\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
